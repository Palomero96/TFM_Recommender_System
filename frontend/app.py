import streamlit as st
import os, sys

# AÃ±ade el directorio src al path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.multiagent_manager import create_graph

# Atributos de la pagina
st.set_page_config(layout="wide", page_title="Recommender System", page_icon="ðŸŽ¯")

# TÃ­tulo llamativo
st.title("ðŸ“šðŸŽ¬ Recommender System")
st.markdown("Haz una consulta y te recomendaremos libros o pelÃ­culas de forma personalizada.")

# Cargamos el grafo 
@st.cache_resource
def init_graph():
    return create_graph()

recommendation_graph = init_graph()

# Se inicializa el historial de mensajes
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Se muestra todo el historial de mensajes
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"], unsafe_allow_html=True)

# Interaccion entre usuario y asistente
if prompt := st.chat_input("Escribe tu consulta aquÃ­..."):
    # Se muestra el mensaje del usuario 
    with st.chat_message("user"):
        st.markdown(f"<b>{prompt}</b>", unsafe_allow_html=True)
    # Se guarda el mensaje del usuario en el historial
    st.session_state.chat_history.append({"role": "user", "content": f"<b>{prompt}</b>"})

    # Mostramos un mensaje mientras se ejecuta un grafo
    with st.spinner("ðŸ¤” Estoy pensando..."):
        result = recommendation_graph.invoke({"input": prompt})
        full_response = result["output"]

    # Se muestra la respuesta del asistente
    with st.chat_message("assistant"):
        st.markdown(full_response, unsafe_allow_html=True)

    # Se guarda el mensaje del asistente en el historial
    st.session_state.chat_history.append({"role": "assistant", "content": full_response})
